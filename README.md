
We introduce adaptive neural networks â€“ greedy $n$-term approximation from a neural dictionary applied to PDEs via the least squares principle. To then solve PDEs using neural networks we need an equivalent extremum problem. Here, we choose the least squares principle of taking the squared residual of the differential equation as well as boundary conditions. The energy inner product associated with the least squares principle then allows us to do two things: explicitly compute the a posteriori error in the energy norm and devise criteria of fit. The criterion of fit will be the loss function we optimize at each iteration to best reduce the current error. Furthermore, we obtain a priori rates of convergence for our algorithm, by applying existing theory on greedy algorithms.